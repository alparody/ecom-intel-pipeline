# Notebook 02 â€“ Silver Layer: Data Cleansing & Typing

from pyspark.sql import SparkSession
from pyspark.sql.functions import col
from pyspark.sql.types import LongType, DoubleType, StringType, TimestampType

# â›½ Start Spark Session
spark = SparkSession.builder.getOrCreate()

# ğŸ“¥ Read from Bronze Layer (CSV files)
bronze_path = "/mnt/bronze/ecommerce_data"
all_files = dbutils.fs.ls(bronze_path)

# Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ù„ÙØ§Øª CSV ÙÙ‚Ø· ÙˆØ­Ø¬Ù…Ù‡Ø§ Ø£ÙƒØ¨Ø± Ù…Ù† ØµÙØ±
valid_csv_files = [f.path for f in all_files if f.path.endswith(".csv") and f.size > 0]

# Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„ØµØ§Ù„Ø­Ø©
df_bronze = spark.read.option("header", True).csv(valid_csv_files)

print(f"âœ… Ø¹Ø¯Ø¯ Ø§Ù„ØµÙÙˆÙ Ù‚Ø¨Ù„ Ø§Ù„ØªÙ†Ø¸ÙŠÙ: {df_bronze.count()}")

# ğŸ§½ ØªÙ†Ø¸ÙŠÙ ÙˆØªØ­ÙˆÙŠÙ„ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
df_silver = df_bronze \
    .withColumn("event_time", col("event_time").cast(TimestampType())) \
    .withColumn("event_type", col("event_type").cast(StringType())) \
    .withColumn("product_id", col("product_id").cast(LongType())) \
    .withColumn("category_id", col("category_id").cast(LongType())) \
    .withColumn("category_code", col("category_code").cast(StringType())) \
    .withColumn("brand", col("brand").cast(StringType())) \
    .withColumn("price", col("price").cast(DoubleType())) \
    .withColumn("user_id", col("user_id").cast(LongType())) \
    .withColumn("user_session", col("user_session").cast(StringType()))

# Ø­Ø°Ù Ø§Ù„ØµÙÙˆÙ Ø§Ù„ØªÙŠ ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ù‚ÙŠÙ… Null Ø­Ø±Ø¬Ø©
df_silver_cleaned = df_silver.dropna(subset=["event_time", "event_type", "product_id", "price"])

print(f"âœ… Ø¹Ø¯Ø¯ Ø§Ù„ØµÙÙˆÙ Ø¨Ø¹Ø¯ Ø§Ù„ØªÙ†Ø¸ÙŠÙ: {df_silver_cleaned.count()}")

# ğŸ“¤ Save to Silver Layer as Parquet
silver_path = "/mnt/silver/ecommerce_data"

df_silver_cleaned.write.mode("overwrite").parquet(silver_path)

print("âœ… ØªÙ… Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙÙŠ Silver Layer Ø¨ØµÙŠØºØ© Parquet")
